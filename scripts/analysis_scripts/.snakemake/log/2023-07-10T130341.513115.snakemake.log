Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job           count    min threads    max threads
----------  -------  -------------  -------------
all               1              1              1
make_umaps        3              1              1
total             4              1              1

Select jobs to execute...

[Mon Jul 10 13:03:42 2023]
rule make_umaps:
    input: ../../analysis/cluster_analysis/Ali-BC/vae_prep/all_samples_merged_vae.h5ad
    output: ../../analysis/cluster_analysis/Ali-BC/best_run_Ali-BC_out/Ali-BC_adata_new.h5ad
    jobid: 2
    wildcards: cohort=Ali-BC
    resources: tmpdir=/tmp

[Mon Jul 10 13:06:34 2023]
Error in rule make_umaps:
    jobid: 2
    output: ../../analysis/cluster_analysis/Ali-BC/best_run_Ali-BC_out/Ali-BC_adata_new.h5ad
    shell:
        python ../make_umaps.py --adata ../../analysis/cluster_analysis/Ali-BC/vae_prep/all_samples_merged_vae.h5ad --cofactor 0.8 --cohort Ali-BC --batch_size 4000 --beta_scheme warmup --hidden_dim_size 32  --latent_dim 20 --random_seed 1234 --n_hidden 2 --checkpoint_dir /home/campbell/sayub/final_hmivae/analysis/cluster_analysis/Ali-BC/hyperparams_tuning/Ali-BC_vae_out_nh2_hd32_ls20_betawarmup_rs1234_bs4000 --output_dir ../../analysis/cluster_analysis/Ali-BC/best_run_Ali-BC_out/ 
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2023-07-10T130341.513115.snakemake.log
